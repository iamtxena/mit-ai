{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg-DvjFCyXSr"
      },
      "source": [
        "# **Facial Emotion Detection**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYdEI31whyE-"
      },
      "source": [
        "---\n",
        "## **Context**\n",
        "---\n",
        "\n",
        "Deep learning has been increasingly applied to tasks involving less structured data types like images, texts, audio, and video in recent years. These endeavors often aim to achieve human-like proficiency in processing such data, leveraging our innate ability to intelligently interact with complex, unstructured information. Within the realm of AI, a field known as Artificial Emotional Intelligence, or Emotion AI, focuses on creating technologies that can understand human emotions by analyzing body language, facial expressions, and voice tones, and respond to them effectively.\n",
        "\n",
        "Recognizing facial expressions plays a vital role in human-computer interaction. Research indicates that facial expressions and other visual signals account for about 55% of how we convey emotions. Thus, developing a model capable of accurately recognizing facial emotions is a significant stride toward equipping machines with AI that exhibits emotionally intelligent behavior. Systems that can automatically recognize facial expressions have a broad range of potential applications, from understanding human behavior to diagnosing psychological conditions, and improving the interaction quality of virtual assistants in customer service settings.\n",
        "\n",
        "---\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "---\n",
        "\n",
        "The goal of this project is to use Deep Learning and Artificial Intelligence techniques to create a computer vision model that can accurately detect facial emotions. The model should be able to perform multi-class classification on images of facial expressions, to classify the expressions according to the associated emotion.\n",
        "\n",
        "---\n",
        "\n",
        "## **Key Questions**\n",
        "\n",
        "---\n",
        "\n",
        "Throughout the project, we will be answering the following questions:\n",
        "\n",
        "- How accurately can the deep learning model identify and classify different facial emotions (happy, sad, surprise, neutral) from images?\n",
        "- How well does the model generalize to new, unseen images? Can it maintain high accuracy across the test, train, and validation datasets?\n",
        "- How does the different model architectures compare in terms of accuracy to classify the different emotions?\n",
        "- What are the potential applications of the developed model, and what implications might its deployment have in some industry fields?\n",
        "\n",
        "---\n",
        "\n",
        "## **Problem Formulation**\n",
        "\n",
        "---\n",
        "\n",
        "We are tasked with leveraging Deep Learning techniques to develop a computer vision model capable of accurately detecting and classifying facial emotions. The model needs to distinguish between four specific emotions (happy, sad, surprise, neutral) based on images of facial expressions. This task involves multi-class classification, requiring the model to predict the correct category of emotion for each image it processes.\n",
        "\n",
        "---\n",
        "\n",
        "## **About the dataset**\n",
        "\n",
        "---\n",
        "\n",
        "The data set consists of 3 folders, i.e., 'test', 'train', and 'validation'.\n",
        "Each of these folders has four subfolders:\n",
        "\n",
        "**‘happy’**: Images of people who have happy facial expressions.<br>\n",
        "**‘sad’**: Images of people with sad or upset facial expressions.<br>\n",
        "**‘surprise’**: Images of people who have shocked or surprised facial expressions.<br>\n",
        "**‘neutral’**: Images of people showing no prominent emotion in their facial expression at all.<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDM89XHyCxrA"
      },
      "source": [
        "## **Mounting the Drive**\n",
        "\n",
        "**NOTE:** Please use Google Colab from your browser for this notebook. **Google.colab is NOT a library that can be downloaded locally on your device.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQi_degJC3dm"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UC8-yLUUCcWh"
      },
      "source": [
        "## **Importing the Libraries**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30fd2144"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "from PIL import Image\n",
        "from typing import List\n",
        "from datetime import datetime\n",
        "\n",
        "# For Data Visualization\n",
        "import seaborn as sns\n",
        "\n",
        "# For Model Building\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.models import Sequential, Model  # Sequential API for sequential model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten  # Importing different layers\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras import backend\n",
        "from tensorflow.keras.optimizers import Adam, SGD  # Optimizers for optimizing the model\n",
        "from tensorflow.keras.callbacks import EarlyStopping  # Regularization method to prevent the overfitting\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from keras.applications.vgg16 import VGG16, preprocess_input\n",
        "from keras.applications.resnet_v2 import ResNet50V2, preprocess_input\n",
        "from keras.applications.efficientnet import EfficientNetB0, preprocess_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCqJk2XpCnJi"
      },
      "source": [
        "### **Let us load and unzip the data**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_syvBdMlDTsr"
      },
      "source": [
        "**Note:**\n",
        "\n",
        "- You must download the dataset from the link provided on Olympus and upload the same on your Google drive before executing the code in the next cell.\n",
        "- In case of any error, please make sure that the path of the file is correct as the path may be different for you.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMfr4tK04C0o"
      },
      "outputs": [],
      "source": [
        "# Storing the path of the data file from the Google drive\n",
        "# path = \"/content/drive/MyDrive/Studies/MIT-AI-ML/11. Capstone Project/Facial_emotion_images.zip\"\n",
        "\n",
        "# The data is provided as a zip file so we need to extract the files from the zip file\n",
        "# with zipfile.ZipFile(path, \"r\") as zip_ref:\n",
        "#    zip_ref.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Preparing the Data**\n",
        "\n",
        "The dataset has three folders, i.e., 'train', 'validation' and 'test'. Each of these folders has four sub-folders, namely 'happy', 'neutral', 'sad', and 'surprise'.\n",
        "\n",
        "We will have the train and test path stored in a variable named 'SUBDIRS', and a base directory 'DATADIR'.\n",
        "\n",
        "The names of the sub-folders, which will be the classes for our classification task will be stored in an array called 'CATEGORIES'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATADIR = \"/home/iamtxena/sandbox/mit-ai/capstone/Facial_Emotion_Recognition/Facial_emotion_images\"\n",
        "# DATADIR = \"/content/Facial_emotion_images\"  # Base directory\n",
        "SUBDIRS = [\"train\", \"validation\", \"test\"]  # Subdirectories\n",
        "CATEGORIES = [\"happy\", \"neutral\", \"sad\", \"surprise\"]  # Emotion categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to check the size of one image, and then check if all the other images have the same size. In case, they are different, we'll resize the ones that are different.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_first_image_size(directory, sub_dirs, categories):\n",
        "    \"\"\"\n",
        "    Returns the size of the first image found in the specified directories.\n",
        "\n",
        "    Parameters:\n",
        "    - directory (str): The base directory of the dataset.\n",
        "    - sub_dirs (list of str): Subdirectories to search through (e.g., ['train', 'validation', 'test']).\n",
        "    - categories (list of str): Categories (e.g., ['happy', 'neutral', 'sad', 'surprise']).\n",
        "\n",
        "    Returns:\n",
        "    - tuple: Size of the first image found (width, height).\n",
        "    \"\"\"\n",
        "    for sub_dir in sub_dirs:\n",
        "        for category in categories:\n",
        "            path = os.path.join(directory, sub_dir, category)\n",
        "            for img_name in os.listdir(path):\n",
        "                img_path = os.path.join(path, img_name)\n",
        "                with Image.open(img_path) as img:\n",
        "                    return img.size  # Return the size of the first image found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the size of the first image\n",
        "expected_size = get_first_image_size(DATADIR, SUBDIRS, CATEGORIES)\n",
        "\n",
        "print(f\"Expected size of the first image: {expected_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_image_sizes(directory, sub_dirs, categories, target_size):\n",
        "    \"\"\"\n",
        "    Checks if all images in the specified directories match the target size.\n",
        "\n",
        "    Parameters:\n",
        "    - directory (str): The base directory of the dataset.\n",
        "    - sub_dirs (list of str): Subdirectories to search through.\n",
        "    - categories (list of str): Categories to search through.\n",
        "    - target_size (tuple): The expected size of the images (width, height).\n",
        "\n",
        "    Returns:\n",
        "    - bool: True if all images match the target size, False otherwise.\n",
        "    \"\"\"\n",
        "    all_match = True  # Flag to keep track of size match\n",
        "\n",
        "    for sub_dir in sub_dirs:\n",
        "        for category in categories:\n",
        "            path = os.path.join(directory, sub_dir, category)\n",
        "            for img_name in os.listdir(path):\n",
        "                img_path = os.path.join(path, img_name)\n",
        "                with Image.open(img_path) as img:\n",
        "                    if img.size != target_size:\n",
        "                        print(f\"Image {img_path} has a different size: {img.size}, expected: {target_size}\")\n",
        "                        all_match = False\n",
        "                        return all_match  # Return early upon first mismatch\n",
        "    return all_match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if all images match the expected size\n",
        "all_match = check_image_sizes(DATADIR, SUBDIRS, CATEGORIES, expected_size)\n",
        "if all_match:\n",
        "    print(\"All images match the expected size.\")\n",
        "else:\n",
        "    print(\"Not all images match the expected size.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpkYTD705Eky"
      },
      "source": [
        "## **Visualizing our Classes**\n",
        "\n",
        "Let's look at our classes.\n",
        "\n",
        "**Write down your observation for each class. What do you think can be a unique feature of each emotion, that separates it from the remaining classes?**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_emotion_images(directory: str, sub_dirs: List[str], emotion: str, image_count: int = 9) -> None:\n",
        "    \"\"\"\n",
        "    Visualizes a specified number of images from a given emotion class directory across specified subdirectories.\n",
        "\n",
        "    Parameters:\n",
        "    - directory (str): The base directory where emotion class folders are located across subdirectories.\n",
        "    - sub_dirs (List[str]): List of subdirectories ('train', 'validation', 'test') to search through.\n",
        "    - emotion (str): The specific emotion class to visualize images from.\n",
        "    - image_count (int): The number of images to display. Defaults to 9.\n",
        "\n",
        "    Returns:\n",
        "    - None: This function does not return any value but displays images inline.\n",
        "    \"\"\"\n",
        "    image_paths: List[str] = []  # To store paths of images to be displayed\n",
        "\n",
        "    # Iterate through the specified subdirectories to collect image paths\n",
        "    for sub_dir in sub_dirs:\n",
        "        emotion_dir: str = os.path.join(directory, sub_dir, emotion)\n",
        "        if os.path.isdir(emotion_dir):\n",
        "            for img_name in os.listdir(emotion_dir):\n",
        "                img_path = os.path.join(emotion_dir, img_name)\n",
        "                image_paths.append(img_path)\n",
        "\n",
        "    # If there are no images found for the emotion, print a message and return\n",
        "    if not image_paths:\n",
        "        print(f\"No images found for the specified emotion: {emotion}\")\n",
        "        return\n",
        "\n",
        "    # Select a random subset of image paths\n",
        "    selected_image_paths: np.ndarray = np.random.choice(image_paths, min(image_count, len(image_paths)), replace=False)\n",
        "\n",
        "    # Setup for plotting\n",
        "    fig = plt.figure(figsize=(4, 4))\n",
        "    columns: int = 3\n",
        "    rows: int = image_count // columns + (1 if image_count % columns else 0)\n",
        "\n",
        "    # Iterate over the selected images and display them\n",
        "    for i, image_path in enumerate(selected_image_paths, start=1):\n",
        "        ax = fig.add_subplot(rows, columns, i)\n",
        "        image = load_img(image_path, target_size=(48, 48))  # Ensure the image is resized to 48x48\n",
        "        plt.imshow(image)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb59IxA35WF-"
      },
      "source": [
        "### **Happy**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBLc1cUpByfa"
      },
      "outputs": [],
      "source": [
        "visualize_emotion_images(DATADIR, SUBDIRS, \"happy\", 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWYioRFM5jMJ"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "- The images appear to be in grayscale and vary in terms of lighting, contrast, and clarity.\n",
        "\n",
        "- The images display a range of happy expressions, from broad smiles showing teeth to subtle smiles without teeth. Also a diversity of subjects in terms of age, gender and also ethnicity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28ZzJwIK6HTH"
      },
      "source": [
        "### **Sad**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4n0oXebe6b0g"
      },
      "outputs": [],
      "source": [
        "visualize_emotion_images(DATADIR, SUBDIRS, \"sad\", 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BpviSLK6mLO"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "- The images capture a wide spectrum of sadness, from subtle, somber expressions to more overt manifestations like crying.\n",
        "- The dataset includes faces with different orientations and features. Some faces are directly looking at the camera, while others are tilted or partially turned away.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D3nRj8x7gjK"
      },
      "source": [
        "### **Neutral**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9bRAog_7qPl"
      },
      "outputs": [],
      "source": [
        "visualize_emotion_images(DATADIR, SUBDIRS, \"neutral\", 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjvfkXFE70Wa"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "- The defining characteristic of these images is the absence of clear, expressive features that denote a specific emotion.\n",
        "- Some faces may have subtle features that could be misconstrued as expressing a mild emotion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avUJfLm08cgt"
      },
      "source": [
        "### **Surprised**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDrkuxNm8mWE"
      },
      "outputs": [],
      "source": [
        "visualize_emotion_images(DATADIR, SUBDIRS, \"surprise\", 9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTnBsUNH_djf"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "- The images showcase a range of intensities of surprise, from wide-eyed and open-mouthed expressions to more subdued, raised-eyebrow looks.\n",
        "- The subjects vary in age, including both infants and adults.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZMfyOH4-YSp"
      },
      "source": [
        "## **Checking Distribution of Classes**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7rCOsTl-HbZ"
      },
      "outputs": [],
      "source": [
        "# Function to count images in each category\n",
        "def count_images(data_dir, categories):\n",
        "    counts = []\n",
        "    for category in categories:\n",
        "        path = os.path.join(data_dir, category)\n",
        "        count = len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))])\n",
        "        counts.append(count)\n",
        "    return counts\n",
        "\n",
        "\n",
        "SUBDIRS_DICT = {\"train\": \"train\", \"validation\": \"validation\", \"test\": \"test\"}\n",
        "\n",
        "# Counting images in each dataset\n",
        "train_counts = count_images(os.path.join(DATADIR, SUBDIRS_DICT[\"train\"]), CATEGORIES)\n",
        "validation_counts = count_images(os.path.join(DATADIR, SUBDIRS_DICT[\"validation\"]), CATEGORIES)\n",
        "test_counts = count_images(os.path.join(DATADIR, SUBDIRS_DICT[\"test\"]), CATEGORIES)\n",
        "\n",
        "\n",
        "# Create DataFrames and format for easier reading\n",
        "def create_df(counts, categories, dataset_name):\n",
        "    df = pd.DataFrame({\"Class\": categories, \"Count\": counts})\n",
        "    df[\"Percentage\"] = (df[\"Count\"] / df[\"Count\"].sum()) * 100\n",
        "    df.set_index(\"Class\", inplace=True)\n",
        "\n",
        "    # Formatting for easier reading\n",
        "    df[\"Count\"] = df[\"Count\"].apply(lambda x: f\"{x:,}\")  # Adds commas to thousands\n",
        "    df[\"Percentage\"] = df[\"Percentage\"].apply(lambda x: f\"{x:.2f}\")  # Rounds to two decimals\n",
        "\n",
        "    print(f\"{dataset_name} Data Distribution:\")\n",
        "    print(df)\n",
        "    total_images = df[\"Count\"].str.replace(\",\", \"\").astype(int).sum()\n",
        "    print(f\"Total images in {dataset_name}: {total_images:,}\\n\")  # Formats total count with commas\n",
        "\n",
        "\n",
        "create_df(train_counts, CATEGORIES, \"Training\")\n",
        "create_df(validation_counts, CATEGORIES, \"Validation\")\n",
        "create_df(test_counts, CATEGORIES, \"Testing\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reZRpnmv8qPL"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "- Training Data: The training dataset shows a relatively balanced distribution among the classes of 'happy', 'neutral', and 'sad', each comprising approximately 26% of the dataset. However, 'surprise' is slightly underrepresented, making up 21% of the data. We'll see on the results if this is noticeable.\n",
        "- Validation Data: In the validation dataset, there's a more pronounced imbalance. 'Happy' expressions dominate at 36.67%, followed by 'neutral' at 24.43%, 'sad' at 22.89%, and 'surprise' at 16.01%. This distribution deviates more significantly from an even split, indicating a potential bias towards 'happy' expressions.\n",
        "- Testing Data: The testing dataset is perfectly balanced, with each class representing 25% of the data. This uniform distribution is ideal for evaluating the model's performance across all classes evenly.\n",
        "\n",
        "Regarding the Exploratory Data Analysis, we can use data augmentation to balance the classes. For now, we'll see the results with the given dataset, and if necessary, we can adjust that.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfJnIxXC80uZ"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- Are the classes equally distributed? If not, do you think the imbalance is too high? Will it be a problem as we progress?\n",
        "- Are there any Exploratory Data Analysis tasks that we can do here? Would they provide any meaningful insights?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7NKTPgdEsgt"
      },
      "source": [
        "## **Creating our Data Loaders**\n",
        "\n",
        "In this section, we are creating data loaders that we will use as inputs to our Neural Network.\n",
        "\n",
        "**You have two options for the color_mode. You can set it to color_mode = 'rgb' or color_mode = 'grayscale'. You will need to try out both and see for yourself which one gives better performance.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d97fee2d"
      },
      "outputs": [],
      "source": [
        "# Set this to 'grayscale' as the images are in grayscale\n",
        "color_mode = \"grayscale\"\n",
        "\n",
        "# As we have checked, all images are 48x48, we will set the img_width and img_height to 48\n",
        "img_width, img_height = 48, 48\n",
        "# A batch size of 32 is appropriate for this dataset provide to provide a good balance\n",
        "# between the model's ability to generalize (avoid overfitting) and computational efficiency.\n",
        "batch_size = 32\n",
        "\n",
        "# Training Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,  # Normalize pixel values to [0,1]\n",
        "    rotation_range=20,  # Slight rotation to introduce variability without distorting emotion features\n",
        "    width_shift_range=0.1,  # Slight horizontal shifts to simulate off-center faces\n",
        "    height_shift_range=0.1,  # Slight vertical shifts to account for different heights/angles\n",
        "    shear_range=0.1,  # Small shear transformations for slight perspective changes\n",
        "    zoom_range=0.1,  # Small zoom in/out to simulate closer or further away faces\n",
        "    horizontal_flip=True,  # Faces are symmetric; flipping can simulate looking from another direction\n",
        "    fill_mode=\"nearest\",  # 'nearest' interpolation for filling in new pixels after a transformation\n",
        ")\n",
        "\n",
        "# Validation and Testing Data should not be augmented!\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "# Assuming train_dir, validation_dir, and test_dir should follow the structure in DATADIR and SUBDIRS\n",
        "train_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"train\"])\n",
        "validation_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"validation\"])\n",
        "test_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"test\"])\n",
        "\n",
        "# Train Generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,  # Set to 'grayscale'\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "# Validation Generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,  # Set to 'grayscale'\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "# Testing Generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,  # Set to 'grayscale'\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,  # shuffle=False to keep data in order for testing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's look at some examples of a batch of augmented training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch a batch of images and labels\n",
        "images, labels = next(train_generator)\n",
        "\n",
        "# Assuming the labels are one-hot encoded, we need to convert them back to class indices\n",
        "labels_indices = labels.argmax(axis=1)\n",
        "\n",
        "# Mapping of indices to class names, based on the 'class_indices' attribute of the generator\n",
        "index_to_class = {v: k for k, v in train_generator.class_indices.items()}\n",
        "\n",
        "fig, axes = plt.subplots(4, 4, figsize=(8, 8))\n",
        "for image, label_index, ax in zip(images, labels_indices, axes.flatten()):\n",
        "    ax.imshow(image.squeeze(), cmap=\"gray\")  # Squeeze and cmap for grayscale\n",
        "    class_name = index_to_class[label_index]\n",
        "    ax.set_title(class_name)\n",
        "    ax.axis(\"off\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qGpQC3q1avy"
      },
      "source": [
        "## **Model Building**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSl8GqRdAcGq"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- Are Convolutional Neural Networks the right approach? Should we have gone with Artificial Neural Networks instead?\n",
        "- What are the advantages of CNNs over ANNs and are they applicable here?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0feec0a7"
      },
      "source": [
        "### **Creating the Base Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Model 1 Architecture:**\n",
        "\n",
        "- First CNN Model will have three convolutional blocks.\n",
        "- Each convolutional block will have a Conv2D layer and a MaxPooling2D Layer.\n",
        "- First Conv2D layer with **64 filters** and a **kernel size of 3x3**. Using the **'same' padding** and providing the **input shape = (48, 48, 1)**. Using **'relu' activation**.\n",
        "- Adding MaxPooling2D layer with **kernel size 3x3** and using **padding = 'same'**.\n",
        "- Adding a second Conv2D layer with **32 filters** and **a kernel size of 3x3**. Using the **'same' padding** and **'relu activation**.\n",
        "- Following it up with another MaxPooling2D layer **kernel size 3x3** and using **padding = 'same'**.\n",
        "- Adding a third Conv2D layer with **32 filters and the kernel size of 3x3**. Using the **'same' padding** and **'relu activation**. Once again, following it up with another Maxpooling2D layer with **kernel size 3x3** and **padding = 'same'**.\n",
        "- Once the convolutional blocks are added, we add the Flatten layer.\n",
        "- Finally, we add dense layers.\n",
        "- Adding first Dense layer with **100 neurons** and **'relu' activation**\n",
        "- The last dense layer needs to have as many neurons as the number of classes, which in this case is 4 and use **'softmax' activation**.\n",
        "- As an optimizer we will use SGD optimizer with **learning rate = 0.01** and **momentum = 0.9**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "backend.clear_session()\n",
        "\n",
        "# Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(42)\n",
        "\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "151077af"
      },
      "outputs": [],
      "source": [
        "# Intializing a sequential model\n",
        "model_1 = Sequential()\n",
        "\n",
        "# Adding first conv layer with 64 filters and kernel size 3x3, padding 'same' provides the output size same as the input size\n",
        "# The input_shape denotes input image dimension\n",
        "model_1.add(Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(48, 48, 1)))\n",
        "\n",
        "# Adding max pooling to reduce the size of output of first conv layer\n",
        "model_1.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
        "\n",
        "# Adding second conv layer with 32 filters and kernel size 3x3, padding 'same' followed by a Maxpooling2D layer\n",
        "model_1.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_1.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
        "\n",
        "# Add third conv layer with 32 filters and kernel size 3x3, padding 'same' followed by a Maxpooling2D layer\n",
        "model_1.add(Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model_1.add(MaxPooling2D((2, 2), padding=\"same\"))\n",
        "\n",
        "# Flattening the output of the conv layer after max pooling to make it ready for creating dense connections\n",
        "model_1.add(Flatten())\n",
        "\n",
        "# Adding a fully connected dense layer with 100 neurons\n",
        "model_1.add(Dense(100, activation=\"relu\"))\n",
        "\n",
        "# Adding the output layer with 3 neurons and activation functions as softmax since this is a multi-class classification problem\n",
        "model_1.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "# Using SGD Optimizer\n",
        "opt = SGD(learning_rate=0.01, momentum=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vgOwCHZxqAlG"
      },
      "source": [
        "### **Compiling and Training the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compiling the model\n",
        "model_1.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Generating the summary of the model\n",
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# results_path = \"/content/drive/MyDrive/Studies/MIT-AI-ML/11. Capstone Project/results\"\n",
        "results_path = \"/home/iamtxena/sandbox/mit-ai/capstone/Facial_Emotion_Recognition/results\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DelayedEarlyStopping(EarlyStopping):\n",
        "    \"\"\"Stop training when a monitored metric has stopped improving after a certain number of epochs.\n",
        "\n",
        "    Arguments:\n",
        "        monitor: Quantity to be monitored.\n",
        "        min_delta: Minimum change in the monitored quantity to qualify as an improvement,\n",
        "                   i.e., an absolute change of less than min_delta will count as no improvement.\n",
        "        patience: Number of epochs with no improvement after which training will be stopped.\n",
        "        verbose: Verbosity mode.\n",
        "        mode: One of `{'auto', 'min', 'max'}`. In `min` mode, training will stop when the\n",
        "              quantity monitored has stopped decreasing; in `max` mode it will stop when the\n",
        "              quantity monitored has stopped increasing; in `auto` mode, the direction is\n",
        "              automatically inferred from the name of the monitored quantity.\n",
        "        baseline: Baseline value for the monitored quantity. Training will stop if the model\n",
        "                  doesn't show improvement over the baseline.\n",
        "        restore_best_weights: Whether to restore model weights from the epoch with the best value\n",
        "                              of the monitored quantity.\n",
        "        start_epoch: The epoch on which to start considering early stopping. Before this epoch,\n",
        "                     early stopping will not be considered. This ensures that early stopping\n",
        "                     checks only after a certain number of epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        monitor=\"val_loss\",\n",
        "        min_delta=0,\n",
        "        patience=0,\n",
        "        verbose=0,\n",
        "        mode=\"auto\",\n",
        "        baseline=None,\n",
        "        restore_best_weights=False,\n",
        "        start_epoch=30,\n",
        "    ):\n",
        "        super(DelayedEarlyStopping, self).__init__(\n",
        "            monitor=monitor,\n",
        "            min_delta=min_delta,\n",
        "            patience=patience,\n",
        "            verbose=verbose,\n",
        "            mode=mode,\n",
        "            baseline=baseline,\n",
        "            restore_best_weights=restore_best_weights,\n",
        "        )\n",
        "        self.start_epoch = start_epoch\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # Override the original `on_epoch_end` method to include `start_epoch` logic.\n",
        "\n",
        "        # If the current epoch is less than the start epoch, skip the early stopping check\n",
        "        if epoch < self.start_epoch:\n",
        "            return\n",
        "\n",
        "        # Call the parent class method to perform the regular early stopping checks after the start epoch\n",
        "        super(DelayedEarlyStopping, self).on_epoch_end(epoch, logs)\n",
        "\n",
        "\n",
        "# Usage of DelayedEarlyStopping\n",
        "# Define the EarlyStopping callback with a start epoch\n",
        "delayed_early_stopping = DelayedEarlyStopping(\n",
        "    monitor=\"val_loss\", patience=10, verbose=1, restore_best_weights=True, start_epoch=30\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87b29701"
      },
      "outputs": [],
      "source": [
        "# Get the current time\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Set up Early Stopping with a patience 7 but acting after at least 30 epochs\n",
        "delayed_early_stopping = DelayedEarlyStopping(\n",
        "    monitor=\"val_loss\", patience=7, verbose=1, restore_best_weights=True, start_epoch=30\n",
        ")\n",
        "\n",
        "# Define the learning rate scheduler callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
        "mc = ModelCheckpoint(\n",
        "    f\"{results_path}/best_model_1_{current_time}.keras\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "# Pulling a single large batch of random validation data for doing the validation after each epoch\n",
        "validationX, validationY = validation_generator.next()\n",
        "\n",
        "# Fitting the model with 40 epochs and using validation set\n",
        "history_1 = model_1.fit(\n",
        "    train_generator,\n",
        "    epochs=40,\n",
        "    validation_data=(validationX, validationY),\n",
        "    callbacks=[delayed_early_stopping, mc, reduce_lr],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting the Training and Validation Accuracies**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history_1.history[\"accuracy\"])\n",
        "plt.plot(history_1.history[\"val_accuracy\"])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR2yf3zH7uje"
      },
      "source": [
        "### **Evaluating the Model on the Test Set**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gffvQXr-70Hm"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of steps for the entire test set to be processed\n",
        "test_steps = test_generator.samples // batch_size\n",
        "\n",
        "# If the number of samples isn't a multiple of the batch size,\n",
        "# you have one more batch with the remaining samples\n",
        "if test_generator.samples % batch_size > 0:\n",
        "    test_steps += 1\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "evaluation_results = model_1.evaluate(test_generator, steps=test_steps)\n",
        "print(f\"Loss: {evaluation_results[0]}, Accuracy: {evaluation_results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting Confusion Matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_probabilities = model_1.predict(test_generator, steps=test_steps)\n",
        "pred = np.argmax(pred_probabilities, axis=1)\n",
        "\n",
        "# Getting the true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Printing the classification report with actual emotion labels\n",
        "print(classification_report(y_true, pred, target_names=CATEGORIES))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SoqluqR-RMbk"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "- As we can see from the above summary, this CNN model has been trained and learned with **144,056 parameters (weights and biases).**\n",
        "- The model's training accuracy shows a progressive increase, reaching about **71.34%**, suggesting the model is learning effectively. The validation accuracy fluctuates but also generally trends upward, peaking at approximately **78.12%**. This divergence might indicate that the model is still learning generalizable features, despite the higher accuracy on the validation set, which can sometimes be attributed to the validation set being slightly 'easier' or containing fewer augmentations.\n",
        "- Upon evaluating the model on the test dataset, it achieved a loss of 0.6081 and an **accuracy of 72.66%**.\n",
        "- From the confusion matrix, the model performs best in identifying 'happy' and 'surprise' emotions. It faces more challenges with 'sad' and 'neutral', in which we have the lowest f1-score, 56% and 63% respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12efb6c8"
      },
      "source": [
        "### **Creating the second Convolutional Neural Network**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypwtou8Zp5EH"
      },
      "source": [
        "### **Model 2 Architecture:**\n",
        "\n",
        "- We plan on having 4 convolutional blocks in this Architecture, each having a Conv2D, MaxPooling2D, and a Dropout layer.\n",
        "- Adding first Conv2D layer with **256 filters** and a **kernel size of 5x5**. Using the **'same' padding** and provide the **input shape = (48, 48, 1)**. Using **'relu' activation**.\n",
        "- Adding MaxPooling2D layer with **kernel size 5x5** and **stride size 2x2**.\n",
        "- Adding a Dropout layer with a dropout ratio of **0.2**.\n",
        "- Adding a second Conv2D layer with **128 filters** and a **kernel size of 5x5**. Using the **'same' padding** and **'relu' activation**.\n",
        "- Following this up with a similar Maxpooling2D layer like above and a Dropout layer with 0.3 dropout ratio.\n",
        "- Adding a third Conv2D layer with **64 filters** and a **kernel size of 5x5**. Using the **'same' padding** and **'relu' activation**.\n",
        "- Following this up with a similar Maxpooling2D layer and a Dropout layer with dropout ratio of 0.3.\n",
        "- Adding a fourth Conv2D layer with **32 filters** and a **kernel size of 3x3**. Using the **'same' padding** and **'relu' activation**.\n",
        "- Following this up with a similar Maxpooling2D layer and a Dropout layer with dropout ratio of 0.3.\n",
        "- Once the convolutional blocks are added, we add the Flatten layer.\n",
        "- Adding first fully connected dense layer with 64 neurons and using **'relu' activation**.\n",
        "- Adding a second fully connected dense layer with 32 neurons and using **'relu' activation**.\n",
        "- Adding a final dense layer with 4 neurons and using **'softmax' activation function**.\n",
        "- Initializing an **Adam optimizer** with a learning rate of 0.0005.\n",
        "- Compiling your model with the optimizer initialized and using **categorical_crossentropy** as the loss function and the 'accuracy' as the metric.\n",
        "- Finally, printing your model summary and writing down the observations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7887b475"
      },
      "outputs": [],
      "source": [
        "backend.clear_session()\n",
        "\n",
        "# Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(42)\n",
        "\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initializing a sequential model\n",
        "model_2 = Sequential()\n",
        "\n",
        "# Adding first conv layer\n",
        "model_2.add(Conv2D(filters=256, kernel_size=(5, 5), padding=\"Same\", input_shape=(48, 48, 1), activation=\"relu\"))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_2.add(Dropout(0.2))\n",
        "\n",
        "# Adding second conv layer\n",
        "model_2.add(Conv2D(filters=128, kernel_size=(5, 5), padding=\"Same\", activation=\"relu\"))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_2.add(Dropout(0.3))\n",
        "\n",
        "# Adding third conv layer\n",
        "model_2.add(Conv2D(filters=64, kernel_size=(3, 3), padding=\"Same\", activation=\"relu\"))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_2.add(Dropout(0.3))\n",
        "\n",
        "# Adding fourth conv layer\n",
        "model_2.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"Same\", activation=\"relu\"))\n",
        "model_2.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_2.add(Dropout(0.3))\n",
        "\n",
        "# Flattening\n",
        "model_2.add(Flatten())\n",
        "model_2.add(Dense(64, activation=\"relu\"))\n",
        "model_2.add(Dense(32, activation=\"relu\"))\n",
        "\n",
        "# Adding the output layer with 4 neurons and activation functions as softmax since this is a multi-class classification problem\n",
        "model_2.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "# Using Adam Optimizer\n",
        "optimizer = Adam(learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2d7wYiTk5uW"
      },
      "source": [
        "### **Compiling and Training the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f79add6"
      },
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model_2.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current time\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Set up Early Stopping with a patience 7 but acting after at least 30 epochs\n",
        "delayed_early_stopping = DelayedEarlyStopping(\n",
        "    monitor=\"val_loss\", patience=7, verbose=1, restore_best_weights=True, start_epoch=30\n",
        ")\n",
        "\n",
        "# Define the learning rate scheduler callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "mc = ModelCheckpoint(\n",
        "    f\"{results_path}/best_model_2_{current_time}.keras\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "# Pulling a single large batch of random validation data for doing the validation after each epoch\n",
        "validationX, validationY = validation_generator.next()\n",
        "\n",
        "# Fitting the model with 40 epochs and using validation set\n",
        "history_2 = model_2.fit(\n",
        "    train_generator,\n",
        "    epochs=40,\n",
        "    validation_data=(validationX, validationY),\n",
        "    callbacks=[delayed_early_stopping, mc, reduce_lr],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting the Training and Validation Accuracies**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history_2.history[\"accuracy\"])\n",
        "plt.plot(history_2.history[\"val_accuracy\"])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VeiN9vSy744e"
      },
      "source": [
        "### **Evaluating the Model on the Test Set**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBNeB-Em7xBy"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of steps for the entire test set to be processed\n",
        "test_steps = test_generator.samples // batch_size\n",
        "\n",
        "# If the number of samples isn't a multiple of the batch size,\n",
        "# you have one more batch with the remaining samples\n",
        "if test_generator.samples % batch_size > 0:\n",
        "    test_steps += 1\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "evaluation_results = model_2.evaluate(test_generator, steps=test_steps)\n",
        "print(f\"Loss: {evaluation_results[0]}, Accuracy: {evaluation_results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting Confusion Matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_probabilities = model_2.predict(test_generator, steps=test_steps)\n",
        "pred = np.argmax(pred_probabilities, axis=1)\n",
        "\n",
        "# Getting the true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Printing the classification report with actual emotion labels\n",
        "print(classification_report(y_true, pred, target_names=CATEGORIES))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MstKA9Op8XOA"
      },
      "source": [
        "**Observations and Insights:**\n",
        "\n",
        "- The second CNN model, equipped with a more complex architecture of **938,948 parameters**, demonstrates a steady learning curve, indicating effective feature learning with a final training accuracy of approximately 69.08%. The **validation accuracy on training** experiences fluctuations but shows an upward trend, with a peak at around **84.37%**.\n",
        "- On testing, the model secured a **loss of 0.5975 and an accuracy of 75%**, which confirms its capability to generalize the learned features to new data reasonably well, aligning closely with the observed validation accuracy.\n",
        "- The confusion matrix reveals the model's proficiency in correctly identifying 'happy' with a high precision of 0.96 and 'surprise' emotions with impressive precision and recall of 0.93 and 0.84, respectively.\n",
        "- In contrast, the model has room for improvement when predicting 'neutral' and 'sad' emotions, as indicated by lower precision and recall scores. 'Neutral' faces have the lowest precision at 0.56, suggesting the model sometimes confuses this emotion with others, while 'sad' faces have a recall of 0.56, indicating that the model fails to detect a significant portion of this emotion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1LQ64iTsSH0"
      },
      "source": [
        "## **Think About It:**\n",
        "\n",
        "- Did the models have a satisfactory performance? If not, then what are the possible reasons?\n",
        "- Which Color mode showed better overall performance? What are the possible reasons? Do you think having 'rgb' color mode is needed because the images are already black and white?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boQi7epI3Lsu"
      },
      "source": [
        "## **Transfer Learning Architectures**\n",
        "\n",
        "In this section, we will create several Transfer Learning architectures. For the pre-trained models, we will select three popular architectures namely, VGG16, ResNet v2, and Efficient Net. The difference between these architectures and the previous architectures is that these will require 3 input channels while the earlier ones worked on 'grayscale' images. Therefore, we need to create new DataLoaders.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kMsvUQuD89r"
      },
      "source": [
        "### **Creating our Data Loaders for Transfer Learning Architectures**\n",
        "\n",
        "In this section, we are creating data loaders that we will use as inputs to our Neural Network. We will have to go with color_mode = 'rgb' as this is the required format for the transfer learning architectures.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWmbGuuRD89r"
      },
      "outputs": [],
      "source": [
        "# Set this to 'rgb' as this is the required format for the transfer learning architectures\n",
        "color_mode = \"rgb\"\n",
        "\n",
        "# Resize our images to the same size expected by the transfer learning architectures\n",
        "img_width, img_height = 224, 224\n",
        "# A batch size of 32 is appropriate for this dataset provide to provide a good balance\n",
        "# between the model's ability to generalize (avoid overfitting) and computational efficiency.\n",
        "batch_size = 32\n",
        "\n",
        "# Training Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_input,  # Use the appropriate preprocessing function\n",
        "    rotation_range=20,  # Slight rotation to introduce variability without distorting emotion features\n",
        "    width_shift_range=0.1,  # Slight horizontal shifts to simulate off-center faces\n",
        "    height_shift_range=0.1,  # Slight vertical shifts to account for different heights/angles\n",
        "    shear_range=0.1,  # Small shear transformations for slight perspective changes\n",
        "    zoom_range=0.1,  # Small zoom in/out to simulate closer or further away faces\n",
        "    horizontal_flip=True,  # Faces are symmetric; flipping can simulate looking from another direction\n",
        "    fill_mode=\"nearest\",  # 'nearest' interpolation for filling in new pixels after a transformation\n",
        ")\n",
        "\n",
        "# Validation and Testing Data should not be augmented!\n",
        "validation_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
        "\n",
        "# Assuming train_dir, validation_dir, and test_dir should follow the structure in DATADIR and SUBDIRS\n",
        "train_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"train\"])\n",
        "validation_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"validation\"])\n",
        "test_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"test\"])\n",
        "\n",
        "# Train Generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "# Validation Generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "# Testing Generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,  # shuffle=False to keep data in order for testing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaUYQdkf7pDG"
      },
      "source": [
        "## **VGG16 Model**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThCSNrWC4HW0"
      },
      "source": [
        "### **Importing the VGG16 Architecture**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "backend.clear_session()\n",
        "\n",
        "# Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(42)\n",
        "\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c83c83e"
      },
      "outputs": [],
      "source": [
        "vgg_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=(img_width, img_height, 3))\n",
        "# Making all the layers of the VGG model non-trainable. i.e. freezing them\n",
        "for layer in vgg_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "vgg_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X76HMyZX4edM"
      },
      "source": [
        "### **Model Building**\n",
        "\n",
        "- Import VGG16 upto the layer of your choice and add Fully Connected layers on top of it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8b123bc6"
      },
      "outputs": [],
      "source": [
        "new_vgg16_model = Sequential()\n",
        "\n",
        "# Adding the convolutional part of the VGG16 model from above\n",
        "new_vgg16_model.add(vgg_model)\n",
        "\n",
        "# Reduces each feature map to a single value by averaging all elements\n",
        "new_vgg16_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Adding a dense output layer\n",
        "new_vgg16_model.add(Dense(512, activation=\"relu\"))\n",
        "new_vgg16_model.add(Dropout(0.5))\n",
        "new_vgg16_model.add(Dense(128, activation=\"relu\"))\n",
        "new_vgg16_model.add(Dropout(0.5))\n",
        "new_vgg16_model.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "# Using Adam Optimizer\n",
        "optimizer = Adam(learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6vK7u7w8GsM"
      },
      "source": [
        "### **Compiling and Training the VGG16 Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86b249f1"
      },
      "outputs": [],
      "source": [
        "new_vgg16_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "new_vgg16_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current time\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Set up Early Stopping with a patience 7 but acting after at least 20 epochs\n",
        "delayed_early_stopping = DelayedEarlyStopping(\n",
        "    monitor=\"val_loss\", patience=7, verbose=1, restore_best_weights=True, start_epoch=20\n",
        ")\n",
        "\n",
        "# Define the learning rate scheduler callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "mc = ModelCheckpoint(\n",
        "    f\"{results_path}/best_model_vgg16_{current_time}.keras\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "# Pulling a single large batch of random validation data for doing the validation after each epoch\n",
        "validationX, validationY = validation_generator.next()\n",
        "\n",
        "# Fitting the model with 40 epochs and using validation set\n",
        "history_vgg = new_vgg16_model.fit(\n",
        "    train_generator,\n",
        "    epochs=40,\n",
        "    validation_data=(validationX, validationY),\n",
        "    callbacks=[delayed_early_stopping, mc, reduce_lr],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting the Training and Validation Accuracies**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history_vgg.history[\"accuracy\"])\n",
        "plt.plot(history_vgg.history[\"val_accuracy\"])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un-19jPckK07"
      },
      "source": [
        "### **Evaluating the VGG16 model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6Y_bSLCkcvr"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of steps for the entire test set to be processed\n",
        "test_steps = test_generator.samples // batch_size\n",
        "\n",
        "# If the number of samples isn't a multiple of the batch size,\n",
        "# you have one more batch with the remaining samples\n",
        "if test_generator.samples % batch_size > 0:\n",
        "    test_steps += 1\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "evaluation_results = new_vgg16_model.evaluate(test_generator, steps=test_steps)\n",
        "print(f\"Loss: {evaluation_results[0]}, Accuracy: {evaluation_results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting Confusion Matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_probabilities = new_vgg16_model.predict(test_generator, steps=test_steps)\n",
        "pred = np.argmax(pred_probabilities, axis=1)\n",
        "\n",
        "# Getting the true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Printing the classification report with actual emotion labels\n",
        "print(classification_report(y_true, pred, target_names=CATEGORIES))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW6kPSky59Gv"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- What do you infer from the general trend in the training performance?\n",
        "- Is the training accuracy consistently improving?\n",
        "- Is the validation accuracy also improving similarly?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuTi6OAx8q_r"
      },
      "source": [
        "**Observations and Insights:\\_\\_**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfC2Kx0v7Sa1"
      },
      "source": [
        "**Note: You can even go back and build your own architecture on top of the VGG16 Transfer layer and see if you can improve the performance**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0Mew4Cc7u7k"
      },
      "source": [
        "## **ResNet V2 Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "backend.clear_session()\n",
        "\n",
        "# Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(42)\n",
        "\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "568a5c9a",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "resnet_model = ResNet50V2(weights=\"imagenet\", include_top=False, input_shape=(img_width, img_height, 3))\n",
        "# Making all the layers of the VGG model non-trainable. i.e. freezing them\n",
        "for layer in resnet_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "resnet_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay4RedCQlL4O"
      },
      "source": [
        "### **Model Building**\n",
        "\n",
        "- Import Resnet v2 upto the layer of your choice and add Fully Connected layers on top of it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "911d3335"
      },
      "outputs": [],
      "source": [
        "new_resnet_model = Sequential()\n",
        "new_resnet_model.add(resnet_model)\n",
        "\n",
        "# Reduces each feature map to a single value by averaging all elements\n",
        "new_resnet_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Adding a dense output layer\n",
        "new_resnet_model.add(Dense(512, activation=\"relu\"))\n",
        "new_resnet_model.add(Dropout(0.5))\n",
        "new_resnet_model.add(Dense(128, activation=\"relu\"))\n",
        "new_resnet_model.add(Dropout(0.5))\n",
        "new_resnet_model.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "# Using Adam Optimizer\n",
        "optimizer = Adam(learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tmtcd1ZElpJy"
      },
      "source": [
        "### **Compiling and Training the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe959789"
      },
      "outputs": [],
      "source": [
        "new_resnet_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "new_resnet_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current time\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Set up Early Stopping with a patience 7 but acting after at least 20 epochs\n",
        "delayed_early_stopping = DelayedEarlyStopping(\n",
        "    monitor=\"val_loss\", patience=7, verbose=1, restore_best_weights=True, start_epoch=20\n",
        ")\n",
        "\n",
        "# Define the learning rate scheduler callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "mc = ModelCheckpoint(\n",
        "    f\"{results_path}/best_model_resnet_{current_time}.keras\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "# Pulling a single large batch of random validation data for doing the validation after each epoch\n",
        "validationX, validationY = validation_generator.next()\n",
        "\n",
        "# Fitting the model with 40 epochs and using validation set\n",
        "history_resnet = new_resnet_model.fit(\n",
        "    train_generator,\n",
        "    epochs=40,\n",
        "    validation_data=(validationX, validationY),\n",
        "    callbacks=[delayed_early_stopping, mc, reduce_lr],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting the Training and Validation Accuracies**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history_resnet.history[\"accuracy\"])\n",
        "plt.plot(history_resnet.history[\"val_accuracy\"])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McHEzBlhxw39"
      },
      "source": [
        "### **Evaluating the ResNet Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEl6IQf0xwci"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of steps for the entire test set to be processed\n",
        "test_steps = test_generator.samples // batch_size\n",
        "\n",
        "# If the number of samples isn't a multiple of the batch size,\n",
        "# you have one more batch with the remaining samples\n",
        "if test_generator.samples % batch_size > 0:\n",
        "    test_steps += 1\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "evaluation_results = new_resnet_model.evaluate(test_generator, steps=test_steps)\n",
        "print(f\"Loss: {evaluation_results[0]}, Accuracy: {evaluation_results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting Confusion Matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_probabilities = new_resnet_model.predict(test_generator, steps=test_steps)\n",
        "pred = np.argmax(pred_probabilities, axis=1)\n",
        "\n",
        "# Getting the true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Printing the classification report with actual emotion labels\n",
        "print(classification_report(y_true, pred, target_names=CATEGORIES))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3htKZRdomiOY"
      },
      "source": [
        "**Observations and Insights:\\_\\_**\n",
        "\n",
        "**Note: You can even go back and build your own architecture on top of the ResNet Transfer layer and see if you can improve the performance.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmnDG4ZbncoR"
      },
      "source": [
        "## **EfficientNet Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "backend.clear_session()\n",
        "\n",
        "# Fixing the seed for random number generators so that we can ensure we receive the same output everytime\n",
        "np.random.seed(42)\n",
        "\n",
        "random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "613c7a71"
      },
      "outputs": [],
      "source": [
        "efficient_model = EfficientNetB0(weights=\"imagenet\", include_top=False, input_shape=(img_width, img_height, 3))\n",
        "# Making all the layers of the efficient_model model non-trainable. i.e. freezing them\n",
        "for layer in efficient_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "efficient_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kDNE8pVngqC"
      },
      "source": [
        "### **Model Building**\n",
        "\n",
        "- Import EfficientNet upto the layer of your choice and add Fully Connected layers on top of it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2b27a6e1"
      },
      "outputs": [],
      "source": [
        "new_efficient_model = Sequential()\n",
        "new_efficient_model.add(efficient_model)\n",
        "\n",
        "# Reduces each feature map to a single value by averaging all elements\n",
        "new_efficient_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "# Adding a dense output layer\n",
        "new_efficient_model.add(Dense(512, activation=\"relu\"))\n",
        "new_efficient_model.add(Dropout(0.5))\n",
        "new_efficient_model.add(Dense(128, activation=\"relu\"))\n",
        "new_efficient_model.add(Dropout(0.5))\n",
        "new_efficient_model.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "# Using Adam Optimizer\n",
        "optimizer = Adam(learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVv4Df_In32Y"
      },
      "source": [
        "### **Compiling and Training the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dc326cd3"
      },
      "outputs": [],
      "source": [
        "new_efficient_model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "new_efficient_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current time\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Set up Early Stopping with a patience 7 but acting after at least 20 epochs\n",
        "delayed_early_stopping = DelayedEarlyStopping(\n",
        "    monitor=\"val_loss\", patience=7, verbose=1, restore_best_weights=True, start_epoch=20\n",
        ")\n",
        "\n",
        "# Define the learning rate scheduler callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "mc = ModelCheckpoint(\n",
        "    f\"{results_path}/best_model_efficient_{current_time}.keras\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "# Pulling a single large batch of random validation data for doing the validation after each epoch\n",
        "validationX, validationY = validation_generator.next()\n",
        "\n",
        "# Fitting the model with 40 epochs and using validation set\n",
        "history_efficient = new_efficient_model.fit(\n",
        "    train_generator,\n",
        "    epochs=40,\n",
        "    validation_data=(validationX, validationY),\n",
        "    callbacks=[delayed_early_stopping, mc, reduce_lr],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting the Training and Validation Accuracies**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history_efficient.history[\"accuracy\"])\n",
        "plt.plot(history_efficient.history[\"val_accuracy\"])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2xjrzYpgoQnN"
      },
      "source": [
        "### **Evaluating the EfficientnetNet Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WJVFenvnoQnN"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of steps for the entire test set to be processed\n",
        "test_steps = test_generator.samples // batch_size\n",
        "\n",
        "# If the number of samples isn't a multiple of the batch size,\n",
        "# you have one more batch with the remaining samples\n",
        "if test_generator.samples % batch_size > 0:\n",
        "    test_steps += 1\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "evaluation_results = new_efficient_model.evaluate(test_generator, steps=test_steps)\n",
        "print(f\"Loss: {evaluation_results[0]}, Accuracy: {evaluation_results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting the confusion matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_probabilities = new_efficient_model.predict(test_generator, steps=test_steps)\n",
        "pred = np.argmax(pred_probabilities, axis=1)\n",
        "\n",
        "# Getting the true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Printing the classification report with actual emotion labels\n",
        "print(classification_report(y_true, pred, target_names=CATEGORIES))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWlk14FOoQnN"
      },
      "source": [
        "**Observations and Insights:\\_\\_**\n",
        "\n",
        "**Note: You can even go back and build your own architecture on top of the VGG16 Transfer layer and see if you can improve the performance.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fk6QAcv5odNF"
      },
      "source": [
        "**Think About It:**\n",
        "\n",
        "- What is your overall performance of these Transfer Learning Architectures? Can we draw a comparison of these models' performances. Are we satisfied with the accuracies that we have received?\n",
        "- Do you think our issue lies with 'rgb' color_mode?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-EH3atQP8q_v"
      },
      "source": [
        "Now that we have tried multiple pre-trained models, let's build a complex CNN architecture and see if we can get better performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjKlBaZDpWoV"
      },
      "source": [
        "## **Building a Complex Neural Network Architecture**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bMFUj3Fpe75"
      },
      "source": [
        "In this section, we will build a more complex Convolutional Neural Network Model that has close to as many parameters as we had in our Transfer Learning Models. However, we will have only 1 input channel for our input images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejGfyYSbtx-F"
      },
      "source": [
        "## **Creating our Data Loaders**\n",
        "\n",
        "In this section, we are creating data loaders which we will use as inputs to the more Complicated Convolutional Neural Network. We will go ahead with color_mode = 'grayscale'.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xj1hM5_ttx-F"
      },
      "outputs": [],
      "source": [
        "# Set this to 'grayscale' as the images are in grayscale\n",
        "color_mode = \"grayscale\"\n",
        "\n",
        "# As we have checked, all images are 48x48, we will set the img_width and img_height to 48\n",
        "img_width, img_height = 48, 48\n",
        "# A batch size of 32 is appropriate for this dataset provide to provide a good balance\n",
        "# between the model's ability to generalize (avoid overfitting) and computational efficiency.\n",
        "batch_size = 32\n",
        "\n",
        "# Training Data Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1.0 / 255,  # Normalize pixel values to [0,1]\n",
        "    rotation_range=20,  # Slight rotation to introduce variability without distorting emotion features\n",
        "    width_shift_range=0.1,  # Slight horizontal shifts to simulate off-center faces\n",
        "    height_shift_range=0.1,  # Slight vertical shifts to account for different heights/angles\n",
        "    shear_range=0.1,  # Small shear transformations for slight perspective changes\n",
        "    zoom_range=0.1,  # Small zoom in/out to simulate closer or further away faces\n",
        "    horizontal_flip=True,  # Faces are symmetric; flipping can simulate looking from another direction\n",
        "    fill_mode=\"nearest\",  # 'nearest' interpolation for filling in new pixels after a transformation\n",
        ")\n",
        "\n",
        "# Validation and Testing Data should not be augmented!\n",
        "validation_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "\n",
        "# Assuming train_dir, validation_dir, and test_dir should follow the structure in DATADIR and SUBDIRS\n",
        "train_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"train\"])\n",
        "validation_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"validation\"])\n",
        "test_dir = os.path.join(DATADIR, SUBDIRS_DICT[\"test\"])\n",
        "\n",
        "# Train Generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,  # Set to 'grayscale'\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n",
        "# Validation Generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    validation_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,  # Set to 'grayscale'\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,  # shuffle=False to keep data in order for evaluation\n",
        ")\n",
        "\n",
        "# Testing Generator\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_dir,\n",
        "    target_size=(img_width, img_height),\n",
        "    batch_size=batch_size,\n",
        "    color_mode=color_mode,  # Set to 'grayscale'\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,  # shuffle=False to keep data in order for testing\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft5U6f1Wie2R"
      },
      "source": [
        "### **Model Building**\n",
        "\n",
        "- Try building a layer with 5 Convolutional Blocks and see if performance increases.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37f9194d"
      },
      "outputs": [],
      "source": [
        "# Initializing a sequential model\n",
        "model_complex = Sequential()\n",
        "\n",
        "# Adding first conv layer\n",
        "model_complex.add(Conv2D(filters=512, kernel_size=(5, 5), padding=\"Same\", input_shape=(48, 48, 1), activation=\"relu\"))\n",
        "model_complex.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_complex.add(Dropout(0.2))\n",
        "\n",
        "# Adding second conv layer\n",
        "model_complex.add(Conv2D(filters=256, kernel_size=(5, 5), padding=\"Same\", activation=\"relu\"))\n",
        "model_complex.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_complex.add(Dropout(0.2))\n",
        "\n",
        "# Adding third conv layer\n",
        "model_complex.add(Conv2D(filters=128, kernel_size=(5, 5), padding=\"Same\", activation=\"relu\"))\n",
        "model_complex.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_complex.add(Dropout(0.3))\n",
        "\n",
        "# Adding fourth conv layer\n",
        "model_complex.add(Conv2D(filters=64, kernel_size=(3, 3), padding=\"Same\", activation=\"relu\"))\n",
        "model_complex.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_complex.add(Dropout(0.3))\n",
        "\n",
        "# Adding fifth conv layer\n",
        "model_complex.add(Conv2D(filters=32, kernel_size=(3, 3), padding=\"Same\", activation=\"relu\"))\n",
        "model_complex.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "model_complex.add(Dropout(0.3))\n",
        "\n",
        "# Flattening\n",
        "model_complex.add(Flatten())\n",
        "model_complex.add(Dense(64, activation=\"relu\"))\n",
        "model_complex.add(Dense(32, activation=\"relu\"))\n",
        "\n",
        "# Adding the output layer with 4 neurons and activation functions as softmax since this is a multi-class classification problem\n",
        "model_complex.add(Dense(4, activation=\"softmax\"))\n",
        "\n",
        "# Using Adam Optimizer\n",
        "optimizer = Adam(learning_rate=0.0005)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fyc0B--hwTHS"
      },
      "source": [
        "### **Compiling and Training the Model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0edabf52"
      },
      "outputs": [],
      "source": [
        "model_complex.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_complex.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the current time\n",
        "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Set up Early Stopping with a patience 7 but acting after at least 30 epochs\n",
        "delayed_early_stopping = DelayedEarlyStopping(\n",
        "    monitor=\"val_loss\", patience=7, verbose=1, restore_best_weights=True, start_epoch=30\n",
        ")\n",
        "\n",
        "# Define the learning rate scheduler callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=5, min_lr=0.00001, verbose=1)\n",
        "\n",
        "mc = ModelCheckpoint(\n",
        "    f\"{results_path}/best_model_complex_{current_time}.keras\",\n",
        "    monitor=\"val_accuracy\",\n",
        "    mode=\"max\",\n",
        "    verbose=1,\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "# Pulling a single large batch of random validation data for doing the validation after each epoch\n",
        "validationX, validationY = validation_generator.next()\n",
        "\n",
        "# Fitting the model with 40 epochs and using validation set\n",
        "history_complex = model_complex.fit(\n",
        "    train_generator,\n",
        "    epochs=40,\n",
        "    validation_data=(validationX, validationY),\n",
        "    callbacks=[delayed_early_stopping, mc, reduce_lr],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting the Training and Validation Accuracies**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(history_complex.history[\"accuracy\"])\n",
        "plt.plot(history_complex.history[\"val_accuracy\"])\n",
        "plt.title(\"model accuracy\")\n",
        "plt.ylabel(\"accuracy\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbAqrAQQVjIR"
      },
      "source": [
        "### **Evaluating the Model on Test Set**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uO26AYRuVm7F"
      },
      "outputs": [],
      "source": [
        "# Calculate the number of steps for the entire test set to be processed\n",
        "test_steps = test_generator.samples // batch_size\n",
        "\n",
        "# If the number of samples isn't a multiple of the batch size,\n",
        "# you have one more batch with the remaining samples\n",
        "if test_generator.samples % batch_size > 0:\n",
        "    test_steps += 1\n",
        "\n",
        "# Evaluating the model on the test set\n",
        "evaluation_results = model_complex.evaluate(test_generator, steps=test_steps)\n",
        "print(f\"Loss: {evaluation_results[0]}, Accuracy: {evaluation_results[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Plotting Confusion Matrix**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_probabilities = model_complex.predict(test_generator, steps=test_steps)\n",
        "pred = np.argmax(pred_probabilities, axis=1)\n",
        "\n",
        "# Getting the true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Printing the classification report with actual emotion labels\n",
        "print(classification_report(y_true, pred, target_names=CATEGORIES))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81uYGSmHwxfD"
      },
      "source": [
        "**Observations and Insights:\\_\\_**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNWc6agwxJ_z"
      },
      "source": [
        "### **Plotting the Confusion Matrix for the chosen final model**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SFTRyIk-yjoQ"
      },
      "outputs": [],
      "source": [
        "pred_probabilities = model_complex.predict(test_generator, steps=test_steps)\n",
        "pred = np.argmax(pred_probabilities, axis=1)\n",
        "\n",
        "# Getting the true labels from the generator\n",
        "y_true = test_generator.classes\n",
        "\n",
        "# Printing the classification report with actual emotion labels\n",
        "print(classification_report(y_true, pred, target_names=CATEGORIES))\n",
        "\n",
        "# Plotting the heatmap using confusion matrix\n",
        "cm = confusion_matrix(y_true, pred)\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\".0f\", xticklabels=CATEGORIES, yticklabels=CATEGORIES)\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGgvpOrP8q_x"
      },
      "source": [
        "**Observations and Insights:\\_\\_**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s_baiF_KllW"
      },
      "source": [
        "## **Conclusion:\\*\\***\\_\\_\\_\\_**\\*\\***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEZPA_mN0tUo"
      },
      "source": [
        "### **Insights**\n",
        "\n",
        "### **Refined insights**:\n",
        "\n",
        "- What are the most meaningful insights from the data relevant to the problem?\n",
        "\n",
        "### **Comparison of various techniques and their relative performance**:\n",
        "\n",
        "- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?\n",
        "\n",
        "### **Proposal for the final solution design**:\n",
        "\n",
        "- What model do you propose to be adopted? Why is this the best solution to adopt?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
