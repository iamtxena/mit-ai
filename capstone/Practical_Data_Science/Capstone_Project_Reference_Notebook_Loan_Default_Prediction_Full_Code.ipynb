{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_iHEvciuTB9"
      },
      "source": [
        "# **Loan Default Prediction**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKyzwpUiuTB2"
      },
      "source": [
        "## **Problem Definition**\n",
        "\n",
        "### **The Context:**\n",
        "\n",
        "- Why is this problem important to solve?\n",
        "\n",
        "### **The objective:**\n",
        "\n",
        "- What is the intended goal?\n",
        "\n",
        "### **The key questions:**\n",
        "\n",
        "- What are the key questions that need to be answered?\n",
        "\n",
        "### **The problem formulation**:\n",
        "\n",
        "- What is it that we are trying to solve using data science?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEHRGpcdo-KO"
      },
      "source": [
        "## **Data Description:**\n",
        "\n",
        "The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable that indicates whether an applicant has ultimately defaulted or has been severely delinquent. This adverse outcome occurred in 1,189 cases (20 percent). 12 input variables were registered for each applicant.\n",
        "\n",
        "- **BAD:** 1 = Client defaulted on loan, 0 = loan repaid\n",
        "\n",
        "- **LOAN:** Amount of loan approved.\n",
        "\n",
        "- **MORTDUE:** Amount due on the existing mortgage.\n",
        "\n",
        "- **VALUE:** Current value of the property.\n",
        "\n",
        "- **REASON:** Reason for the loan request. (HomeImp = home improvement, DebtCon= debt consolidation which means taking out a new loan to pay off other liabilities and consumer debts)\n",
        "\n",
        "- **JOB:** The type of job that loan applicant has such as manager, self, etc.\n",
        "\n",
        "- **YOJ:** Years at present job.\n",
        "\n",
        "- **DEROG:** Number of major derogatory reports (which indicates a serious delinquency or late payments).\n",
        "\n",
        "- **DELINQ:** Number of delinquent credit lines (a line of credit becomes delinquent when a borrower does not make the minimum required payments 30 to 60 days past the day on which the payments were due).\n",
        "\n",
        "- **CLAGE:** Age of the oldest credit line in months.\n",
        "\n",
        "- **NINQ:** Number of recent credit inquiries.\n",
        "\n",
        "- **CLNO:** Number of existing credit lines.\n",
        "\n",
        "- **DEBTINC:** Debt-to-income ratio (all your monthly debt payments divided by your gross monthly income. This number is one way lenders measure your ability to manage the monthly payments to repay the money you plan to borrow.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcZcGaZruTB-"
      },
      "source": [
        "## **Import the necessary libraries and Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pvj9AGJtuTB_"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import scipy.stats as stats\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        ")\n",
        "from sklearn import metrics\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "sns.set_theme()\n",
        "\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9ykJzCRuTCD"
      },
      "source": [
        "## **Data Overview**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLgLExFVBZtg"
      },
      "source": [
        "- Reading the dataset\n",
        "- Understanding the shape of the dataset\n",
        "- Checking the data types\n",
        "- Checking for missing values\n",
        "- Checking for duplicated values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "za7znZ1cBZtg"
      },
      "outputs": [],
      "source": [
        "hm = pd.read_csv(\"hmeq.csv\")\n",
        "# Copying data to another variable to avoid any changes to original data\n",
        "data = hm.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Assuming 'data' is a pandas DataFrame containing your dataset\n",
        "\n",
        "# Understanding the shape of the dataset\n",
        "print(\"Shape of the dataset:\")\n",
        "print(data.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking the data types\n",
        "print(\"Data types of each column:\")\n",
        "print(data.dtypes)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking for duplicated values\n",
        "print(\"Number of duplicated records:\")\n",
        "print(data.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cols = data.select_dtypes([\"object\"]).columns.tolist()\n",
        "\n",
        "# Adding target variable to this list as this is a classification problem and the target variable is categorical\n",
        "cols.append(\"BAD\")\n",
        "\n",
        "# Changing the data type of object type column to category using astype() function\n",
        "for i in cols:\n",
        "    data[i] = data[i].astype(\"category\")\n",
        "\n",
        "# Checking the info again and the datatype of different variables\n",
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHTODkjLuTCT"
      },
      "source": [
        "## Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hRbpzF0BZth"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g51vGZAGuTCT"
      },
      "source": [
        "- Observations from Summary Statistics\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZcMbNvZuTCW"
      },
      "source": [
        "## **Exploratory Data Analysis (EDA) and Visualization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Td9x7DdBZti"
      },
      "source": [
        "- EDA is an important part of any project involving data.\n",
        "- It is important to investigate and understand the data better before building a model with it.\n",
        "- A few questions have been mentioned below which will help you approach the analysis in the right manner and generate insights from the data.\n",
        "- A thorough analysis of the data, in addition to the questions mentioned below, should be done.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba_dqV0sBZti"
      },
      "source": [
        "**Leading Questions**:\n",
        "\n",
        "1. What is the range of values for the loan amount variable \"LOAN\"?\n",
        "2. How does the distribution of years at present job \"YOJ\" vary across the dataset?\n",
        "3. How many unique categories are there in the REASON variable?\n",
        "4. What is the most common category in the JOB variable?\n",
        "5. Is there a relationship between the REASON variable and the proportion of applicants who defaulted on their loan?\n",
        "6. Do applicants who default have a significantly different loan amount compared to those who repay their loan?\n",
        "7. Is there a correlation between the value of the property and the loan default rate?\n",
        "8. Do applicants who default have a significantly different mortgage amount compared to those who repay their loan?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65yxFJFVuTCW"
      },
      "source": [
        "### **Univariate Analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2zXcgmBeBZti"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.describe(include=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def histogram_boxplot(data, feature, figsize=(15, 7), bins=None):\n",
        "    \"\"\"\n",
        "    Custom function for plotting a histogram and a boxplot for a numerical variable,\n",
        "    with a vertical line indicating the mean and its value.\n",
        "    \"\"\"\n",
        "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw={\"height_ratios\": (0.15, 0.85)}, figsize=figsize)\n",
        "    sns.boxplot(data=data, x=feature, ax=ax_box)\n",
        "    ax_box.set(xlabel=\"\")\n",
        "    if bins:\n",
        "        sns.histplot(data=data, x=feature, ax=ax_hist, bins=bins, kde=True)\n",
        "    else:\n",
        "        sns.histplot(data=data, x=feature, ax=ax_hist, kde=True)\n",
        "    ax_hist.set(ylabel=\"Frequency\")\n",
        "\n",
        "    # Mean value\n",
        "    mean_value = data[feature].mean()\n",
        "\n",
        "    # Add a vertical line for the mean\n",
        "    ax_hist.axvline(mean_value, color=\"r\", linestyle=\"--\")\n",
        "    ax_box.axvline(mean_value, color=\"r\", linestyle=\"--\")\n",
        "\n",
        "    # Annotate the mean value on the plot\n",
        "    ax_hist.annotate(\n",
        "        f\"Mean: {mean_value:.2f}\",\n",
        "        xy=(mean_value, 0),\n",
        "        xycoords=(\"data\", \"axes fraction\"),\n",
        "        xytext=(0, -30),\n",
        "        textcoords=\"offset points\",\n",
        "        va=\"top\",\n",
        "        ha=\"center\",\n",
        "        color=\"red\",\n",
        "    )\n",
        "\n",
        "    plt.title(f\"Univariate Analysis of {feature} (Numerical)\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def univariate_analysis(data):\n",
        "    \"\"\"\n",
        "    Performs univariate analysis with appropriate plots for numerical and categorical variables.\n",
        "    - data: DataFrame containing the data\n",
        "    \"\"\"\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Separate the dataset into numerical and categorical data\n",
        "    numerical_data = data.select_dtypes(include=[\"int64\", \"float64\"])\n",
        "    categorical_data = data.select_dtypes(include=[\"object\", \"category\"])\n",
        "\n",
        "    # Numerical Data Analysis\n",
        "    for column in numerical_data.columns:\n",
        "        histogram_boxplot(data, column)  # Use the custom function for numerical variables\n",
        "\n",
        "    # Categorical Data Analysis\n",
        "    for column in categorical_data.columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        total = float(len(data[column]))\n",
        "        ax = sns.countplot(\n",
        "            x=column, data=categorical_data, palette=\"Set2\", order=categorical_data[column].value_counts().index\n",
        "        )\n",
        "        plt.title(f\"Univariate Analysis of {column} (Categorical)\")\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Adding percentage annotations\n",
        "        for p in ax.patches:\n",
        "            percentage = \"{:.1f}%\".format(100 * p.get_height() / total)\n",
        "            x = p.get_x() + p.get_width() / 2\n",
        "            y = p.get_height()\n",
        "            ax.annotate(percentage, (x, y), ha=\"center\", va=\"bottom\")\n",
        "\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "univariate_analysis(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg5IFtbouTCa"
      },
      "source": [
        "### **Bivariate Analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9b60NSTMBZtj"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def bivariate_analysis(data, target=\"BAD\"):\n",
        "    sns.set(style=\"whitegrid\")\n",
        "\n",
        "    # Continuous Variables\n",
        "    continuous_columns = data.select_dtypes(include=[\"int64\", \"float64\", \"float32\"]).columns\n",
        "    continuous_columns = continuous_columns.drop(target) if target in continuous_columns else continuous_columns\n",
        "\n",
        "    for column in continuous_columns:\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.boxplot(x=target, y=column, data=data)\n",
        "        plt.title(f\"{column} vs. {target}\")\n",
        "        plt.show()\n",
        "\n",
        "    # Categorical Variables\n",
        "    categorical_columns = data.select_dtypes(include=[\"object\", \"category\"]).columns\n",
        "    if target not in categorical_columns:\n",
        "        categorical_columns = categorical_columns.union([target])\n",
        "\n",
        "    for column in categorical_columns:\n",
        "        if column == target:\n",
        "            continue\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        ax = sns.countplot(x=column, hue=target, data=data)\n",
        "        plt.title(f\"{column} vs. {target}\")\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Calculate percentages and add annotations\n",
        "        total = len(data[column])\n",
        "        for p in ax.patches:\n",
        "            percentage = \"{:.1f}%\".format(100 * p.get_height() / total)\n",
        "            x = p.get_x() + p.get_width() / 2\n",
        "            y = p.get_height()\n",
        "            ax.annotate(percentage, (x, y), ha=\"center\", va=\"bottom\")\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    # Example for Continuous Variables Comparison with Regression Line\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.lmplot(x=\"LOAN\", y=\"MORTDUE\", hue=target, data=data, aspect=1.5)\n",
        "    plt.title(\"LOAN vs. MORTDUE with Regression Line\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# bivariate_analysis(data, 'BAD')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "bivariate_analysis(data, \"BAD\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pc9wZJcGuTCm"
      },
      "source": [
        "### **Multivariate Analysis**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLk0Fgx-BZtj"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def multivariate_analysis(data, target=\"BAD\"):\n",
        "    sns.set(style=\"white\")\n",
        "\n",
        "    # Correlation Heatmap for Numerical Variables\n",
        "    numerical_data = data.select_dtypes(include=[\"int64\", \"float64\", \"float32\"])\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    correlation_matrix = numerical_data.corr()\n",
        "    sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.5)\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "    # Pairplot for the dataset with 'BAD' as hue\n",
        "    # Note: For large datasets, consider using a sample to speed up the plotting\n",
        "    # or select fewer columns if the pairplot is too crowded or slow to generate\n",
        "    sampled_data = data.sample(frac=0.1, random_state=42)  # Sample 10% of the data for the pairplot if necessary\n",
        "    sns.pairplot(sampled_data, hue=target, vars=numerical_data.columns)\n",
        "    plt.title(\"Pairplot with BAD as hue\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage\n",
        "multivariate_analysis(data, \"BAD\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEjMlq0quTCp"
      },
      "source": [
        "## Treating Outliers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTtFHgKTBZtj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "def treat_outliers(data, method=\"cap\"):\n",
        "    \"\"\"\n",
        "    Treat outliers in the numerical columns of the dataset based on the IQR method.\n",
        "\n",
        "    Parameters:\n",
        "    - data: pandas DataFrame containing the data.\n",
        "    - method: 'cap' to cap outliers with threshold values or 'remove' to drop rows with outliers.\n",
        "\n",
        "    Returns:\n",
        "    - The DataFrame with outliers treated.\n",
        "    \"\"\"\n",
        "    treated_data = data.copy()\n",
        "    for column in treated_data.select_dtypes(include=[\"float64\", \"float32\", \"int64\"]).columns:\n",
        "        Q1 = treated_data[column].quantile(0.25)\n",
        "        Q3 = treated_data[column].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "        if method == \"cap\":\n",
        "            treated_data[column] = np.where(treated_data[column] < lower_bound, lower_bound, treated_data[column])\n",
        "            treated_data[column] = np.where(treated_data[column] > upper_bound, upper_bound, treated_data[column])\n",
        "        elif method == \"remove\":\n",
        "            treated_data = treated_data[(treated_data[column] >= lower_bound) & (treated_data[column] <= upper_bound)]\n",
        "\n",
        "    return treated_data\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# data_treated = treat_outliers(data, method='cap')\n",
        "# or\n",
        "# data_treated = treat_outliers(data, method='remove')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data = treat_outliers(data, method=\"cap\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8CEq24hBZtj"
      },
      "source": [
        "## Treating Missing Values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDYGPOlXBZtk"
      },
      "outputs": [],
      "source": [
        "# Understanding the shape of the dataset\n",
        "print(\"Shape of the dataset:\")\n",
        "print(data.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking the data types\n",
        "print(\"Data types of each column:\")\n",
        "print(data.dtypes)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(data.isnull().sum())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking for duplicated values\n",
        "print(\"Number of duplicated records:\")\n",
        "print(data.duplicated().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Understanding the shape of the dataset\n",
        "print(\"Shape of the dataset:\")\n",
        "print(data_transformed_df.shape)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking the data types\n",
        "print(\"Data types of each column:\")\n",
        "print(data_transformed_df.dtypes)\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking for missing values\n",
        "print(\"Missing values in each column:\")\n",
        "print(data_transformed_df.isnull().sum())\n",
        "print(\"\\n\")\n",
        "\n",
        "# Checking for duplicated values\n",
        "print(\"Number of duplicated records:\")\n",
        "print(data_transformed_df.duplicated().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG_XM04vuTCs"
      },
      "source": [
        "## **Important Insights from EDA**\n",
        "\n",
        "What are the the most important observations and insights from the data based on the EDA performed?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkV_0337BZtk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cdqhtr8yuS-L"
      },
      "source": [
        "## **Model Building - Approach**\n",
        "\n",
        "- Data preparation\n",
        "- Partition the data into train and test set\n",
        "- Build the model\n",
        "- Fit on the train data\n",
        "- Tune the model\n",
        "- Test the model on test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CGW7uh5BZtk"
      },
      "source": [
        "### Logistic Regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fs0QKb4ABZtk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9oYAeptGBZtk"
      },
      "source": [
        "### Decision Tree\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nfvaEVqsBZtk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdO4E2btpoPD"
      },
      "source": [
        "### **Decision Tree - Hyperparameter Tuning**\n",
        "\n",
        "- Hyperparameter tuning is tricky in the sense that **there is no direct way to calculate how a change in the hyperparameter value will reduce the loss of your model**, so we usually resort to experimentation. We'll use Grid search to perform hyperparameter tuning.\n",
        "- **Grid search is a tuning technique that attempts to compute the optimum values of hyperparameters.**\n",
        "- **It is an exhaustive search** that is performed on the specific parameter values of a model.\n",
        "- The parameters of the estimator/model used to apply these methods are **optimized by cross-validated grid-search** over a parameter grid.\n",
        "\n",
        "**Criterion {“gini”, “entropy”}**\n",
        "\n",
        "The function to measure the quality of a split. Supported criteria are “gini” for the Gini impurity and “entropy” for the information gain.\n",
        "\n",
        "**max_depth**\n",
        "\n",
        "The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n",
        "\n",
        "**min_samples_leaf**\n",
        "\n",
        "The minimum number of samples is required to be at a leaf node. A split point at any depth will only be considered if it leaves at least min_samples_leaf training samples in each of the left and right branches. This may have the effect of smoothing the model, especially in regression.\n",
        "\n",
        "You can learn about more Hyperpapameters on this link and try to tune them.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4x88goTBZtk"
      },
      "source": [
        "### **Building a Random Forest Classifier**\n",
        "\n",
        "**Random Forest is a bagging algorithm where the base models are Decision Trees.** Samples are taken from the training data and on each sample a decision tree makes a prediction.\n",
        "\n",
        "**The results from all the decision trees are combined together and the final prediction is made using voting or averaging.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64OtDSUSqbcD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAx1Ooocp72i"
      },
      "source": [
        "### **Random Forest Classifier Hyperparameter Tuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UQIxCZcqJHa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPywjJo6uS-9"
      },
      "source": [
        "**1. Comparison of various techniques and their relative performance based on chosen Metric (Measure of success):**\n",
        "\n",
        "- How do different techniques perform? Which one is performing relatively better? Is there scope to improve the performance further?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mtOvTtEY7sM"
      },
      "source": [
        "**2. Refined insights:**\n",
        "\n",
        "- What are the most meaningful insights relevant to the problem?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNJHd0R7Y7sM"
      },
      "source": [
        "**3. Proposal for the final solution design:**\n",
        "\n",
        "- What model do you propose to be adopted? Why is this the best solution to adopt?\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
